%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Imperial Placement Report Template
% LaTeX Template
% Version 1.0 (28/06/16)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper,12pt,titlepage]{article}
\usepackage[left=3cm,right=3cm,top=3.5cm,bottom=3cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[toc,page]{appendix}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{color}
\usepackage[maxbibnames=99]{biblatex}
\usepackage{csquotes}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{smartdiagram}

\usetikzlibrary{ arrows, positioning, quotes, shapes, shadows, trees}

\usepackage{listings}

\usepackage[most]{tcolorbox}
\usepackage{inconsolata}

\newtcblisting[auto counter]{sexylisting}[2][]{sharp corners,
    fonttitle=\bfseries, colframe=black, listing only,
    listing options={basicstyle=\fontsize{11}{13}\selectfont\ttfamily,language=csh,columns=spaceflexible},
    title=Listing \thetcbcounter: #2, #1}

\bibliography{library}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textit{\nouppercase{\rightmark}}}
\fancyhead[R]{\thepage}

\setlength{\parindent}{0em}
\renewcommand{\baselinestretch}{1.5}

\begin{document}
\tikzstyle{every node}=[draw=black,thick,anchor=west]
\tikzstyle{selected}=[draw=red,fill=red!30]
\tikzstyle{optional}=[dashed,fill=gray!50]
\tikzstyle{abstract}=[rectangle, draw=black, rounded corners, fill=blue!40, drop shadow,
        text centered, anchor=north, text=white, text width=3cm]
\tikzstyle{comment}=[rectangle, draw=black, rounded corners, fill=green, drop shadow,
        text centered, anchor=north, text=white, text width=3cm]
\tikzstyle{myarrow}=[->, >=open triangle 90, thick]
\tikzstyle{decision} = [diamond, draw, fill=blue!20,
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20,
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
\setlength{\topmargin}{0in}
\center % Center everything on the page


%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Imperial College London}\\[1.5cm] % Name of your university/college
\textsc{\Large Department of Computing}\\[0.5cm] % Major heading such as course name

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Drone Delivery Simulation\\with SpatialOS}\\[0.4cm] % Title of your document
\HRule \\[0.4cm]

%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Paul \textsc{Balaji} \\
\end{flushleft}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Prof.~William \textsc{Knottenbelt}
\end{flushright}
\begin{flushright} \large
\emph{Second Marker:} \\
Prof.~Kin \textsc{Leung}
\end{flushright}
\end{minipage}\\[1cm]

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[0.5cm] % Date, change the \today to a set date if you want to be precise


\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\renewcommand{\abstractname}{\large Abstract}
\begin{abstract}
test post pls ignore
\end{abstract}

\renewcommand{\abstractname}{\large Acknowledgements}
\begin{abstract}
ack
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}
Drone technology is becoming increasingly popular. Their agility and ability to be used remotely makes them ideal for a number of use cases in several industries such as film, law enforcement, emergency services, agriculture and commercial delivery\cite{Koontz}.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/drone_interest_over_time_google.png}
  \caption{Google Trends for ``Drone Technology''. \cite{Google2018}}
  \label{fig:starship_citizen}
\end{figure}

Due to numerous advances in technology, drones are quickly advancing to the point where human input is no longer a necessity. This has led to many companies showing interest in integrating drones with their work in the coming years. \\

Although it may be an engineer's dream for a fully automated world, drones in particular are a harrowing reminder that there are real risks associated with them. There are already several incidents of drones crashing into planes and flying into areas they shouldn't, most notably near Heathrow airport\cite{BBCNews2017}. All of this provides motivation to introduce some form of autonomous air traffic control system to navigate these drones to their respective destinations in a safe manner.\\

However, prior work has been done on the routing and navigation aspect of such a system\cite{Balaji2017}. In order for drones to truly take over more aspects of our lives, we must look at how they can provide a tangible benefit to specific use-cases. There is no doubt that simply removing the human element can save costs drastically, and there is none more exciting an application than with physical delivery networks.

The main factors that could prompt higher adoption of drones for delivery networks are cost, delivery speeds, and convenience. As figure \ref{fig:drone_del_cost} shows, drones can be both faster and cheaper than existing deliver options that consumers have access to.
\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/ark-invest-drones-analysis-768x460.png}
  \caption{How Amazon Air fits into the wider delivery market. \cite{Wang}}
  \label{fig:drone_del_cost}
\end{figure}

It appears then that Amazon stands to increase their margins considerably if they can successfully pull off their Prime Air initiative\cite{Welch2015}, but to do this they need to be able to handle the additional problems of scheduling and load balancing massive quantities of drones, on top of the existing routing problems. \\

In a more general sense, goods delivery networks have a lot to gain by optimising for profit, and to do this better we consider a variable delivery pricing mechanism that factors in the quality of service provided to the user. A high-priority product may warrant a higher delivery fee for quicker delivery but a similarly high penalty for being late. At the same time, a low-priority product may not have any such penalty, instead opting for a low delivery fee. Being able to factor this new fee model when scheduling tasks and allocating drones may be the next step in the ongoing automation of last-mile delivery\cite{Joerss2016}.

Regardless of how such a model is implemented, it needs to be tested thoroughly before being deployed into production with actual drones. Considering a typical delivery drone could cost around \$100 to \$500\cite{Menon2013}, and that there is likely be a lot of failure before a working implementation, there could be several hundreds of thousand dollars worth of losses. \\

Therefore, our best port of call is to simulate scenarios that are as close to the real world as possible. One tech firm investigating how to produce richer, meaningful and more realistic simulations is \textit{Improbable}\cite{ImprobableWorldsLtd.2018a} - a London-based tech startup actively developing a distributed simulation platform called \textit{SpatialOS}\cite{ImprobableWorldsLtd.2018b}. \\

We aim to leverage the power of SpatialOS to produce novel simulations of a drone delivery network. The simulation will build upon existing routing solutions\cite{Balaji2017} and would aim to schedule deliveries based on a Quality of Service value curve. Furthermore, we wish to show how the profitability of a drone delivery network with this form of scheduling might compare to delivery networks that utilise a different scheduling mechanism.

%**********************************************%
\newpage
\section{Background}
We first provide an insight into new developments in the physical delivery network sector, and then specifically considerations to account for when integrating drone technology in our day to day lives. Additionally we summarise prior work completed by Imperial students on an autonomous air traffic control system for drones. \\

We then continue to discuss methods of prioritising a delivery network for financial gain. Finally, we give details about Improbable's SpatialOS and the reasoning for using this platform for the drone simulation.

\subsection{Future of Delivery Networks}
From the ancient days of Assyrian trade in 19 BCE \cite{stearns2001the}, to the modern day online market - the ability to trade goods and services has had a profound impact on the way we live our lives. Thanks to globalisation and the explosion of the internet, we are now able to use a service such as Amazon Prime and get goods delivered within days, if not hours, of purchasing through a simple click. Through it all, physical delivery networks enable this online retail behemoth. \\

As technology evolves, so too will these delivery networks - and many companies are currently looking into how to leverage upcoming tech\cite{Lee2016} in order to generate hype for their business and, more importantly, increase their profit margins. Some promising developments are delivery robots, autonomous cars and, of course, drones.

\subsubsection{Delivery Robots}
Delivery robots are unmanned devices that are designed to be operated on pavements and cycle lanes at low speeds of about 4 miles per hour\cite{Lee2016}. Although the unmanned device market is in its infancy, there are already prototypes hitting the market such as Starship's robot\cite{Laris2016}, which can hold upto 20-25 pounds, and Dispatch's ``Carry''\cite{Kokalitcheva2016}, that holds up to 100 pounds.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/starship.jpg}
  \caption{Starship's robot confronting a senior citizen. Image from Engadget\cite{Souppouris2015}.}
  \label{fig:starship_citizen}
\end{figure}

Due to their on-the-ground nature, delivery robots are primarily designed for low-footfall suburban neighbourhoods, closed residential areas or campuses. They would be scheduled through an Uber-like application that may allow users to track the location of the robot, for the customer's knowledge, and to also unlock the robot once it reaches its destination\cite{Hohenadel2015}. \\

Thanks to their similarities to autonomous cars, they may have regulatory advantages compared to drones because much of the legislation coming through for autonomous vehicles can apply to delivery robots as well. However, there are profitability concerns because each robot can cost hundres of dollars and there is at this stage there is a high risk of failure, damage and even theft. \\

Despite some drawbacks, it is evident these robots are designed for short journeys in the so-called "first-mile delivery" sector - with exciting use cases being the delivery of flowers, groceries and possibly medication.

\subsubsection{Autonomous Vehicles}
Once the de-facto icon of science fictional futurism, autonomous cars are now a close reality. Companies such as Tesla are doing pioneering work in bringing self-driving cars\cite{Tesla2018} to the general consumer. Meanwhile, other businesses like Uber are experimenting with autonmous taxi services\cite{Gibbs2017}. \\

In fact, Google has obtained a patent for an autonomous delivery truck \cite{Hall-Geisler} with different compartments that could be unlocked by validated package recipients - much akin to an Amazon Locker-on-wheels. If not this, these vehicles could be used as an autonomous courier to pick-up locations that users could visit to retrieve their packages.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=0.4\linewidth]{img/google_truck_patent.jpeg}
  \caption{Illustration of Google's Delivery Truck. \cite{google_truck_patent}}
  \label{fig:google_truck_patent}
\end{figure}

Although current regulations dictate that a driver must remain present in the vehicle in the event of software performing differently to expections, this is a necessary precaution whilst driverless systems are still being tested - with further amendments to these laws in sight once these systems can be deemed safe for public use\cite{Bowcott2017}.

\subsubsection{Drones}
The concept of drones has been around for decades, particularly in the military, but only in recent times has the technology evolved to a point where anyone can walk into a consumer electronics store and purchase a drone for their own private use. Though these are manned, it is admirable that many witness the benefits of drones in film/television\cite{Verrier2015}, agriculture\cite{Jarman2016}, policing\cite{BBCNews2017a} and other exotic activities\cite{Roberts2016}. \\

Being able to fly allows drones to not be held hostage to traffic jams or congestion - after all, the sky is a far greater expanse than even the most intricate of road networks. Not only that, they can literally travel ``as the crow flies'' - allowing them to access remote and previously hard-to-reach areas.\\

Despite several near-misses \cite{Davies2017} and accidents\cite{BBCNews2016} at just Heathrow airpot alone, engineering teams across the world are working on ways to improve drone visibility to its surroundings - one such technology being automatic dependent surveillance-broadcast (ADS-B) \cite{Dillow2015}. Drones with ADS-B work by determining their own geolocation and velocity in order to broadcast this back to the global network.\\

ADS-B is already widely used in aircraft, enabling flight-tracking services such as flightradar24 to exist\cite{Flightradar242018}, but recent developments have allowed the ADS-B units to be compact and cost-effective enough to be suitable for drones. As more drones adopt this technology, instead of having to get instant updates from a drone directly, autonomous drone traffic control systems could make use of the shared global network for their scheduling and routing calculations. \\

Over time, an increasing percentage of the world's drones will likely join these shared autonomous systems. This could enable different delivery networks to manage their own fleets as their own air traffic controllers, all the while making use of the shared pool of readable data to identify, intercept and avoid drone collisions at the earliest stage in their pathfinding as possible.

\subsection{Amazon Prime}
Amazon Prime is a paid subscription service that Amazon offers to users, giving unlimited access to their video and music streaming services as well as providing free same-day and next-day delivery on a large majority of goods sold on their platform. The Prime service applies to products that are either sold directly by Amazon or fulfilled on behalf of a third-party merchant.

\subsubsection{Fulfilment by Amazon}
One of the services Amazon provides to merchants is the \textit{Fulfilment by Amazon} service, where goods are held at Amazon depots until a user purchases a product. The fee paid by the merchant for Amazon to take care of delivery depends on the duration of time the product is held by Amazon as well as the type of package and weight of the product.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=0.9\linewidth]{img/fulfilled_by_amazon_table.png}
  \caption{Fulfilment fees for standard-size items on Amazon UK. \cite{Channel2017}}
  \label{fig:fulfilled_by_amazon_table}
\end{figure}

An incentive for merchants to join this scheme is to attract customers with the same-day and next-day delivery that Amazon Prime offers. This allows them to piggy-back off the existing delivery network that Amazon has developed for significantly lower fees (Figure \ref{fig:fulfilled_by_amazon_table}) than if they were to store the package and source a distributor for themselves.

\subsubsection{Amazon Prime Air}
In December of 2013, Amazon announced their intentions to research and develop their own drone delivery service dubbed \textit{Amazon Prime Air} \cite{Amazon.comInc.2013}. Amazon intends this service to deliver packages within 30 minutes of an order being placed, with a maximum package weight of 5 pounds (approximately 2.27 kg).\\

Following this, in November of 2015 the company enlisted the help of Jeremy Clarkson to give a closer look at a more developed drone prototype \cite{Amazon.comInc.2015b}, describing how Prime Air would fit into the lives of modern families. Just over a year later, Amazon released a video demonstrating a fully autonomous delivery using Prime Air \cite{Amazon.comInc.2016}.

\begin{figure}[!hbpt]
  \centering
  \includegraphics[width=0.6\textwidth]{img/amazon_prime_air_drone.png}
  \caption{One of Amazon's Prime Air prototypes. \cite{Amazon.comInc.}}
  \label{fig:prime_air_example}
\end{figure}

One of the many roadblocks to scale this technology out across the country is regulatory approval. Since drones are a new technology that improve in leaps and bounds each year, authorities and enterprises are careful to make hasty judgements that either hamper drone adoption or cause civilian harm. To this end, Amazon are also suggesting proposals to how airspace can be best utilised by drones. \cite{Amazon.comInc.2015} \cite{Amazon.comInc.2015a}

\subsection{Drone Considerations}
Drones stand to be a revolutionary part of our lives as we welcome the new, incoming era of automation. However, to be practical there are a few key concepts one must understand to ensure that they remain a help and not a hinderence to mankind.

\subsubsection{No Fly Zones}
No Fly Zones (which we will abbreviate as NFZs) are geographical areas where a drone is not allowed to enter or fly at any altitude. Examples of these may include Hyde Park, Buckingham Palace, airports and military locations. Typically these are static obstacles that will always remain a NFZ, however we could also consider cases when they could be created dynamically.\\

Suppose there was an issue of national security, it would be beneficial for the security and intelligence services to set up a temporary NFZ around areas where they deem a risk so that they can conduct their own operations without worry of external parties interfering with the situation.

\subsubsection{Other Drones}
Naturally we would want to make sure that we avoid colliding into other drones as well. In a perfect system, a single air traffic controller would have totalitarian dominance over all drones that take to the skies - however this is not \textit{Black Mirror} and we have to assume that there will always be drones that this system will not be able to control or predict. \\

Nonetheless, a system can ensure that it navigates drones under its control as best as it can, avoiding these external drones and other rogue drones that may be flying with the sole intention of causing problems to others.

\subsubsection{Manned Aviation}
Manned Aviation is any form of airborne transport with humans on board. This would include passenger jets, private jets, helicopters and other miscallaneous vechicles. For simulation purposes, we can consider these to have straight paths from a start to an end because relative to the zig-zagging of drones - they are effectively straight. \\

It is paramount to avoid collisions with manned aviation because there is a high risk of human calamity, in addition to the bad press and financial costs associated with such an air traffic incident.

\subsubsection{Toll Zones}
Toll Zones are geographical areas where a drone has to pay an extra fee to pass through at any altitude. It is a very similar idea to the one that led to Congestion Charges being applied to much of central London. By restricting certain areas only to those who are willing to pay the fee to use the airspace, it reduces the density of air traffic in a specific area. These charges could also be used as an incentive to reduce pollution in the toll zone, and the additional revenue generated by the toll fees could be used towards the drone-related systems in place within the zone.

\subsection{Autonomous Air Traffic Control (AATC)}
\subsubsection{What is AATC?}
During the Autumn Term of the 2016-17 academic year, several students undertook a group project in association with Microsoft and Altitude Angel to produce an autonomous air traffic control system. The goal of the project was to safely navigate drones from their start to their end goals, whilst avoiding obstacles and taking the shortest possible path to minimise battery use.

\subsubsection{Technical Overview}
AATC has a simple client-server architecture, where drones connect to the REST service to send their telemetry information and goal waypoints every second. The server sends back direction recommendations to navigate the drone to its destination. \\

Because it would have been too expensive to trial the system on actual drones, a test bench was also implemented that would simulate the drones polling the server and responding to its recommendations. This test bench produced a set of simulation data, that is then available to view on the AATC visualiser. \cite{Balaji2017a}

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/aatc_tech_overview.jpg}
  \caption{Technical Overview of AATC. \cite{Balaji2017}}
  \label{fig:aatc_tech_overview}
\end{figure}

The system was designed with three challenges in mind. The first challenge was to route drones from their origins to their destinations, and secondly, to ensure they avoided collisions with both static and dynamic obstacles - such as the ones mentioned in Section 2.1. The last challenge is to design the system in such a way that it would be able to scale to hundreds and thousands of drones. \\

\newpage
To this end, a \textit{Global Layer} was built to deal with static obstacles (such as NFZs) by calculating a path for the drone around NFZs before it begins its journey. The second problem was tackled with a \textit{Reactive Layer} that handles non-static obstacles (such as manned aviation and other drones). This is the layer that would be providing the real-time updates to drones as they poll the server every second. \\

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=0.8\linewidth]{img/edrone.jpg}
  \caption{The default AATC drone model. \cite{Balaji2017}}
  \label{fig:edrone}
\end{figure}

Only one drone type was used in AATC, with its specification outlined above in Figure \ref{fig:edrone}. It has a radius of 1m and a maximum velocity in any direction of 5 metres per second.

\subsection{The Global Layer}
The Global Layer holds the static representation of the world so that given a start and end, it can compute an optimal set of waypoints that a drone should follow - thereby dealing with AATC's path-finding problem.

\subsubsection{Dijkstra's Algorithm}
Dijkstra's algorithm is a popular algorithm to find the shortest paths between nodes in a graph. When using a co-ordinate grid system, each coordinate could represent a node in a graph and thus the algorithm can also be used to find the shortest path between a source and destination. \\

However, in the real world, we also have to consider the cost of computation and potentially make use of heuristics in order to return the shortest path given a limited amount of time. Especially in the drone case, we want to compute a good path as quick as possible in order to let the drone proceed along its way.

\subsubsection{A* Algorithm}
Enter, the A* algorithm. This algorithm is a generalisation of Dijkstra's algorithm that reduces the number of nodes explored during the search process by use of a heuristic - typically a minimum distance to the destination. A benefit over Dijkstra in particular is that it considers the distance already traveled into account, which aids the heuristic mechanism. \\

Naturally, by searching less nodes on a graph, less computation is performed and therefore A* has better performance than just using a pure form of Dijkstra's algorithm.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/search_comparison.png}
  \caption{Comparing popular search algorithms. \cite{Balaji2017}}
  \label{fig:search_comparison}
\end{figure}

But for all these positive aspects, one must remember that our use for this algorithm is to compute paths in a real world, which is not necessarily split up into a nice, clean grid. So we turn our attention to a modification of the A* algorithm: Theta*.

\subsubsection{Theta* Algorithm}
The Theta* algorithm is an any-angle pathfinding algorithm based on the A* algorithm that introduces Line of Sight (LOS) checks, which means that each jump from node to node in the returned path can be at any angle and not just up, down, left or right. This neat addition allows Theta* to be capable of finding near-optimal paths with a runtime comparable to A* \cite{Uras2015}. \\

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/a_star_vs_optimal.png}
  \caption{A* vs Optimal Path. \cite{Balaji2017}}
  \label{fig:a_star_vs_optimal}
\end{figure}

Being able to get as short a path as possible is absolutely vital in the drone use-case because they have limited flying time. After all, their batteries can only last so long before they need to be recharged. Therefore, it is important to ensure drones travel as little distance as possible in order to maximise the number of times they can be used between charges.

\newpage
\subsubsection{Lazy Theta* Algorithm}
A further optimisation is to reduce the number of LOS checks performed, as found in another paper by the original authors of the Theta* algorithm\cite{Nash2010}. This algorithm assumes every node is in line of sight of its parent, and the LOS check is only performed once the child node is expanded on. If this turns out to be false, then the algorithm defaults to a typical A* approach. Reducing the number of LOS checks therefore improves the algorithm's overall performance.

\subsubsection{Conclusions}
As seen on Table \ref{tab:aatc_exec_time}, A* has far better execution times than both Theta* variants but when we get to Table \ref{tab:aatc_total_drone_distance} it is evident that Lazy/Theta* produces better paths because the total distance travelled by drones is fewer. Therefore, Lazy Theta* was chosen as the core algorithm for the Global Layer.

\begin{table}[!hbpt]
\centering
\begin{tabular}{|l|c|c|}
\hline
\multicolumn{1}{|c|}{Test Case} & A* & \multicolumn{1}{l|}{Theta* and Lazy Theta Star} \\ \hline
Queen Threat & 6.11 & 5.98 \\ \hline
The Imperial Tunnel & 10.83 & 10.22 \\ \hline
The Great London Beehive & 52.63 & 49.77 \\ \hline
The Nightmare of Hyde Park & 118.4 & 115.32 \\ \hline
The Great Wall of Imperial College & 16.37 & 15.46 \\ \hline
Multi Drone collision & 15.27 & 14.82 \\ \hline
\end{tabular}
\caption{Total path distances for all drones (in km). \cite{Balaji2017}}
\label{tab:aatc_total_drone_distance}
\end{table}

\begin{table}[!hbpt]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Test Case                          & A*                                                          & \multicolumn{1}{l|}{Theta*}                                   & \multicolumn{1}{l|}{Lazy Theta*}                              \\ \hline
Imperial Tunnel                    & \begin{tabular}[c]{@{}c@{}}118\\ 120\\ 121\end{tabular}     & \begin{tabular}[c]{@{}c@{}}141\\ 138\\ 137\end{tabular}       & \begin{tabular}[c]{@{}c@{}}124\\ 132\\ 129\end{tabular}       \\ \hline
Imperial Tunnel Mean               & 119.7                                                       & 138.7                                                         & 128.3                                                         \\ \hline
The Great London Beehive (GLB)           & \begin{tabular}[c]{@{}c@{}}859\\ 916\\ 894\end{tabular}     & \begin{tabular}[c]{@{}c@{}}1318\\ 1486\\ 1555\end{tabular}    & \begin{tabular}[c]{@{}c@{}}1166\\ 1075\\ 1201\end{tabular}    \\ \hline
The GLB Mean                       & 889.7                                                       & 1453                                                          & 1147.4                                                        \\ \hline
The Nightmare of Hyde Park (NHP)        & \begin{tabular}[c]{@{}c@{}}9771\\ 9687\\ 10118\end{tabular} & \begin{tabular}[c]{@{}c@{}}28436\\ 27970\\ 28273\end{tabular} & \begin{tabular}[c]{@{}c@{}}21330\\ 24786\\ 21609\end{tabular} \\ \hline
The NHP Mean                       & \multicolumn{1}{l|}{9858.7}                                 & \multicolumn{1}{l|}{28226.4}                                  & 22575                                                         \\ \hline
The Great Wall Of Imperial College (GWIC) & \begin{tabular}[c]{@{}c@{}}394\\ 447\\ 416\end{tabular}     & \begin{tabular}[c]{@{}c@{}}913\\ 910\\ 912\end{tabular}       & \begin{tabular}[c]{@{}c@{}}838\\ 840\\ 841\end{tabular}       \\ \hline
The GWIC Mean                      & 419                                                         & 911.7                                                         & 839.7                                                         \\ \hline
\end{tabular}
\caption{Execution times for pathfinding algorithms on AATC test cases. \cite{Balaji2017}}
\label{tab:aatc_exec_time}
\end{table}

\newpage
\subsection{The Reactive Layer}
Given the list of waypoints that the Global Layer generates, the Reactive Layer's task is to provide the right speed and direction to drones whilst taking into consideration any dynamic obstacles that the drone may face, such as manned aviation, other drones and potentially dynamic NFZs.

\subsubsection{Artificial Potential Fields (APF)}
A novel way to approach this layer is to create an Artificial Potential Field (APF) in the area the drone operates in, and give each point in this field a potential\cite{Zhu2016}. By having a low potential for the destination and large potentials at obstacles, the drone simply has to move in a way to get to point of lowest potential. It is analogous to a magnet in a magnetic field, repelled by obstacles and attracted to its destination. \\

Although one might argue that the Global Layer is not needed anymore because the Reactive Layer solves the pathfinding problem, there are a few issues that arise with this idea. As Figure \ref{fig:apf_comp} shows, a U-shaped object poses a problem because the drone may be attracted to the destination and then quickly repelled by the obstacle, and then back to being attracted - and this cycle is seemingly endless.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=0.9\linewidth]{img/obst.png}
  \caption{APF with obstacles (green peaks) and a destination (blue trough). \cite{Balaji2017}}
  \label{fig:obst}
\end{figure}

By introducing a random element, as in Rotating APF, the object is repelled in a slightly different direction each time to make it out of the trap and eventually reach its destination. Even this method, however, does not actually guarantee that the drone makes it past the U-shaped obstacle because it depends on how the random element behaves and also whether the drone has enough battery to be loitering for long.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/apf_comp.png}
  \caption{Pure APF vs Rotating APF vs AATC Implementation. \cite{Balaji2017}}
  \label{fig:apf_comp}
\end{figure}

Clearly, integrating both layers proves most fruitful - as the waypoints generated by the Global Layer are used to quite literally navigate around the problematic aspects of the Reactive Layer. This integration that was implemented in the AATC project.

\subsubsection{Equations for APF}
The full set of equations used to obtain the recommended unnormalized velocity $v_{raw}$ of the drone is:
$$U_a = \rho_ad_{goal}$$
$$U_r =  \begin{cases}
      \rho_r\frac{1}{d_{obst} - d_{safe}} & d_{obst} \leq d_{influence}   \\
      0 & otherwise \\

   \end{cases}
$$
$$U_{ret} = \rho_{ret}d_{last}$$
$$U = U_a + U_r + U_{ret}$$
$$\bm{v_{raw}} = \nabla U$$

where $\nabla U$ is the gradient of U, and U is the magnitude of potential at a point\cite{Balaji2017}.

Since $\nabla U$ is the recommended velocity, it can be computed from first principles:
$$\nabla U(x,y,z) = \colvec{3}
                {\frac{\delta U(x,y,z)}{\delta x}}
                {\frac{\delta U(x,y,z)}{\delta y}}
                {\frac{\delta U(x,y,z)}{\delta z}}
                  \approx \colvec{3}
                {U(x + 1,y,z) -  U(x,y,z)}
                {U(x,y + 1,z) -  U(x,y,z)}
                {U(x,y,z + 1) -  U(x,y,z)}$$

The recommended velocity can now be computed by first calculating the potential at 4 points. To calculate the potential at a given point, one must find the sum of:
\begin{itemize}
  \item $U_a$ - attraction potential using euclidean distance to next waypoint.
  \item $U_r$ - repulsion potential using euclidean distance to nearest obstacle.
  \item $U_{ret}$ - return potential using euclidean distance to point from last time step.
\end{itemize}

To allow drones to gently come to their goals instead of shooting past and potentially missing, the recommended speed is calculated by taking the minimum of the drone's max speed and the distance to the goal:
$$ speed = min(max\_speed, d_{goal}) $$

The gradient $\nabla U$ is then normalised to be a unit vector before being multiplied by the speed to compute the final velocity.

\subsubsection{Genetic Algorithm}
By paying close attention to Section 2.5.2, we can identify several undefined constants: $\rho_r$, $\rho_a$, $\rho_{ret}$ and $d_{influence}$. The balance between them is key, because if $\rho_a$ is too high relative to $\rho_r$ then the drones ignore obstacles and die on impact, but if $\rho_r$ is too high then they will oscillate between objects and never reach their goal. After much fiddling about with "magic numbers", a genetic algorithm was employed to provide the optimal values for the specific drone model. \\

Genetic algorithms are a metaheuristic inspired by the process of natural selection, and they are a way by which high-quality solutions can be generated for optimisation problems\cite{Mitchell1996}. In the case of AATC, the aforementioned constants need to be fine-tuned in order to produce an APF model that, to its best ability, does not return absurd, erroneous velocities. See Figure \ref{fig:aatc_spaghett} as an example. \\

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/aatc_spaghett.jpg}
  \caption{Drone paths after increasing repulsion constant by 3 orders of magnitude.}
  \label{fig:aatc_spaghett}
\end{figure}

Effectively, an initial range of values (see Table \ref{tab:gen_alg}) is provided and the genetic algorithm cycles through all of these, letting these constants compete against each other to see which set of constants produces the simulation with the lowest cost. This cost is determined from a cost function that returns the remaining battery life of a drone if it reaches its destination, or a relatively huge number otherwise.
$$cost = \sum_{testcase} \sum_{drone}
    \begin{cases}
      batteryUsed & if \; drone \; reached \; destination    \\
      100000 & otherwise \\

   \end{cases}$$

\begin{table}[!hbpt]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
Constant           & Min & Max & Step & Total \\ \hline
Attraction         & 0.8 & 1.1 & 0.1    & 3     \\ \hline
Repulsion          & 100 & 500 & 100  & 5     \\ \hline
Return             & 0.1 & 0.7 & 0.2  & 4     \\ \hline
Influence distance & 300 & 600 & 100  & 4     \\ \hline
\multicolumn{4}{|l|}{Population}      & 240   \\ \hline
\end{tabular}
\caption{Initial configuration of the genetic algorithm for AATC. \cite{Balaji2017}}
\label{tab:gen_alg}
\end{table}

\subsubsection{Conclusions}
In the end, the genetic algorithms only improved the simulations by about 2\%\cite{Balaji2017} which could mean that the original range of constants was already a good set to work with. However, there is still scope for even greater improvements if the genetic algorithm was to be run with a greater number of test scenarios, bigger range of values to cycle through, and generally larger simulations.

\subsection{Scheduling Algorithms}
In this section we look at a few well known CPU scheduling algorithms\cite{Bell2018}. Although CPU scheduling is not part of this project, being able to understand a few different approaches to the problem will give insight into how to schedule drones later on. However, there are two key difference between drone and CPU scheduling.\\

The first is that CPUs can time slice different tasks whereas once a drone is scheduled onto a task, it has to complete the whole task before being able to pick up a new one. Secondly, there are vastly more drones that could be running at a point in time compared to the number of cores in multi-core CPU, leading to far greater parallelism.

\subsubsection{First Come First Serve (FCFS)}
As the title says, this algorithm operates as a FIFO queue where tasks are scheduled in the same order that they arrive. Some might consider this the fairest form of scheduling, since it does not discriminate against any task, but simply prioritises the one that arrived soonest.

\subsubsection{Shortest Job First (SJF)}
This approach looks at the time each incoming task would take to complete, and puts the shortest task at the front of the schedule. While this means that small tasks get completed quickly, a constant stream of small tasks could mean that any heavier tasks never get scheduled or completed.

\subsubsection{Priority Scheduling}
Priority scheduling is a more general case of SJF, since each task is now given a priority - defined either internally or externally - and tasks with the highest priority are scheduled first. In SJF, the inverse of the estimated job time is used as the priority.

\subsubsection{Multi-Level Feedback Queue (MLFQ)}
MLFQ is an extension of an ordinary multi-level queue, where there are several queues of tasks to be completed and tasks are scheduled from non-empty queues in a round-robin fashion. A benefit of this approach is that each queue could be running different scheduling algorithms that better fits the priority of that queue.\\

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=0.6\linewidth]{img/mlqs.jpg}
  \caption{Multiple levels of queues, scheduled in a round-robin fashion. \cite{Bell2018}}
  \label{fig:mlqs}
\end{figure}

The ``feedback'' element of MLFQ allows tasks to be moved from one queue to another, depending on changes of circumstances. For example, if a queue has been left in a low priority queue for long enough, it may be moved to a higher priority queue - thus improving the chance that all incoming tasks get scheduled at some point in time.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=0.6\linewidth]{img/mlfqs.jpg}
  \caption{Arrows indicate tasks can be moved up one queue to another. \cite{Bell2018}}
  \label{fig:mlfqs}
\end{figure}

\subsection{REVISIT: Time Value}
Thus far, we have shown that whilst there is great scope to utilise drones for delivery of physical goods, the existing implementation of AATC only touches upon the routing aspect - how to get from point A to point B. Even in all the test cases, the drones' start and end points were hand-picked by the developers\cite{Balaji2017} to give an insight into how AATC would operate in a variety of scenarios.\\

To gain greater understanding of how drones can positively impact industries, simulations need to be performed which take into account the financial aspect of their use case. For example when it comes to drone delivery networks, we want to ensure that the drones not only reach their destination quickly, but that they also get scheduled and utilised in the appropriate manner to maximise the profitability of the network.

\subsubsection{Time-Value of Money}
As described by Investopedia \cite{Investopedia}, the \textit{Time-Value of Money} is ``the concept that money available at the present time is worth more than the identical sum in the future due to its potential earning capacity''. This draws from the idea that investors would prefer to receive the same amount of money sooner rather than later, in order to obtain interest from the sum or reinvest it for greater growth.

\subsubsection{Time-Value of Data}
By a similar train of thought, there is the concept of the \textit{Time-Value of Data} - whereby the value to a business of some data decays over time and it is best to gain insight from it as soon as possible. This idea is often used in business intelligence, where companies may have large sets of unstructured data that need to be processed and analysed to derive value for the business. For example, a retailer would rather know what its users are interested in now rather than a month ago, so that they can tailor their sales and offers to leverage customer interest.

\newpage
\subsection{SpatialOS}
SpatialOS is a platform, created by \textit{Improbable}\cite{ImprobableWorldsLtd.2018a}, for running massive-scale simulated worlds. In their own words\cite{ImprobableWorldsLtd.2018b}:
\begin{displayquote}
SpatialOS is a cloud-based computational platform that lets you use many servers and engines to power a single world. The platform coordinates a swarm of micro-services called workers, which overlap and dynamically reorganize to power a huge, seamless world. The platform also lets you handle a huge number of concurrent players across different devices in one world.
\end{displayquote}

Although most commonly used for large distributed game worlds, SpatialOS has been used for simulating cities\cite{Narula2017}, the backbone of the Internet\cite{ImprobableWorldsLtd.2016} and a model of the brain\cite{ImprobableWorldsLtd.2018}. It is an ideal platform for simulating ``independent entities that have a location in space''\cite{ImprobableWorldsLtd.2018}.

\begin{figure}[!hbpt]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{img/spatialos-diagram_current2.png}
  \caption{The current approach.}
  \label{fig:spatialos-diagram_current2}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{img/spatialos-diagram_solution2.png}
  \caption{How SpatialOS is different.}
  \label{fig:spatialos-diagram_solution2}
\end{subfigure}
\caption{Comparing approaches to multiplayer. \cite{ImprobableWorldsLtd.2018b}}
\label{fig:test}
\end{figure}

Anybody is able to sign up and use SpatialOS to build games using either the Unity3D or Unreal game engines. Alternatively, one can use the C\#, C++ and Java SDKs to create custom workers to be ran on SpatialOS deployments. \\

A benefit of SpatialOS's Unity and Unreal SDKs is that creating a sole game client, without manually writing network code, is almost enough to get multiplayer built-in to the game for ``free'', i.e. taken as granted.

\subsubsection{Unity SDK}
Of the two game engine integrations, the Unity SDK is arguably the most stable because it has been in development for far longer than the Unreal SDK, and it is not in beta. From prior industrial experience, the Unity integration is also easier to implement quick prototypes with and iterate on.

\subsubsection{Abstraction}
SpatialOS adopts an \textbf{Entity}-\textbf{Component}-\textbf{Worker} model.
\begin{itemize}
  \item \textbf{Entities} are anything in the world that have a position.
  \item \textbf{Components} define state and how other entities interact with them.
  \item \textbf{Workers} are micro-services that simulate components of entities in the world.
\end{itemize}

\subsubsection{Developer Tools}
There is extensive logging of performance metrics, which is a fantastic starting point for scaling up a simulation\cite{Brighting}. In addition to this, there is an active developer \textbf{Forum} - where SpatialOS developers and Improbable engineers meet to help other SDK users solve their problems\cite{ImprobableWorldsLtd.2018c}. \\

Naturally as the public SDKs have matured, so too have the public-facing developer tools. Currently, there is an \textbf{Inspector} to view the location of entities and the values that their components have at a given time, in either a local or deployed simulation.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/io_inspector.png}
  \caption{The Inspector demonstrating a deployment of the iOS Demo, \textit{Quest}. \cite{ImprobableWorldsLtd.2017}}
  \label{fig:bg_scalable_zones}
\end{figure}

\subsubsection{Layered Simulation}
By virtue of games having several dynamic systems all interlinking with each other, it is possible to create layered simulations on SpatialOS. This works well with the concept of Global, Reactive and Zonal layers that have been mentioned already in Sections 2.4, 2.5 and 2.6. Furthermore, as SpatialOS is distributed by nature, it is the ideal platform to create and execute massive, layered, distributed simulations.


%**********************************************%
\newpage
\section{REVISIT: Translating AATC to SpatialOS}
Before even thinking about simulating orders and scheduling deliveries, there must be a backbone in place that allows drones to get from a start to end point. To this end, the pathfinding and collision avoidance capabilities of AATC (Section 2.4) will be the backbone of the simulation.

\subsection{Basic Interaction Model}
However, AATC was designed as a simple master-slave system such that the server assumed total control over each drone in the system. To produce a scalable delivery network, modifications have to made that allows entities to be decoupled and not limited to the constraints imposed by the existing AATC model.

\subsubsection{Drones}
As a testament to hardware and software improvements over time, modern consumer drones are now fully capable of avoiding obstacles \cite{DJI} and navigating to GPS locations on their own \cite{DJIb}. By taking this into account we can have drones responsible for their own movement, leading to greater autonomy in the simulated delivery network, in turn enabling the system to scale up to bigger areas with vastly more drones than AATC.\\

\begin{figure}[!hbpt]
 \centering
 \begin{tikzpicture}[
 node distance = 2mm and 55mm,
             > = Stealth,
    box/.style = {rectangle, draw, thick, fill=white,
                   text width=2cm, minimum height=3cm,
                   align=center,  drop shadow}
                    ]
 \node [box](sp)                {Drone};
 \node [box,right=of sp] (sc)   {Server};
 \draw [->] ([yshift=3mm] sp.east)  to ["ping every second"] ([yshift=3mm] sc.west) ;
 \draw [->] ([yshift=-3mm] sc.west) to ["updated velocity"] ([yshift=-3mm] sp.east);
 \end{tikzpicture}
 \caption{Drone-Server Interaction in AATC.}
 % \label{fig:test}
 \end{figure}

In the AATC case, drones would have to ping the server every second to request a suggested velocity for the drone. This means that the controller would have to compute new velocities for every single drone it controls, every second.

This is a lot of compute that can be offloaded to the drone such as in our proposed model, where we simply provide drones their next waypoint and let them navigate to it by themselves. The only pings to the server now would be to request a new waypoint or to retrieve an updated list of nearby static obstacles.

 \begin{figure}[!hbpt]
  \centering
  \begin{tikzpicture}[
  node distance = 2mm and 55mm,
              > = Stealth,
     box/.style = {rectangle, draw, thick, fill=white,
                    text width=2cm, minimum height=3cm,
                    align=center,  drop shadow}
                     ]
  \node [box](sp)                {Drone};
  \node [box,right=of sp] (sc)   {Controller};
  \draw [->] ([yshift=3mm] sp.east)  to ["ping for waypoint"] ([yshift=3mm] sc.west) ;
  \draw [->] ([yshift=-3mm] sc.west) to ["next waypoint"] ([yshift=-3mm] sp.east);
  \end{tikzpicture}
  \caption{Updated Drone-Controller Interaction.}
  % \label{fig:proposed_drone_controller_interaction}
  \end{figure}

This new Drone-Controller dynamic means that drones need to be responsible for avoiding dynamic obstacles and other physical entities all by themselves. Essentially, the AATC Reactive Layer has been split from the server and will now reside on each drone. The drone must periodically ping a controller for details of the nearest static obstacle, and combine this information with data from its own sensors in order to effectively avoid both static and dynamic obstacles.

\subsubsection{Controllers}
Much like an air traffic control tower, controllers exist as entities responsible for guiding drones in a particular region of the world. As the collision avoidance Reactive Layer will now run on the drones, the controller will be running the pathfinding Global Layer and keep a bitmap representation of the world.\\

Since drones only know about their next waypoint, it is the responsiblity of the controller to know which waypoint a drone last visited and to return the correct next waypoint when requested, which could involve checking if the drone is actually at the waypoint it claims to be at. The controller should also return information on the closest static obstacle to the drone at a given point.

\subsection{Representing Entities}
\subsubsection{Unity SDK}
As we are working with the SpatialOS Unity SDK, we must become familiar with the concept of a \textit{Prefab}, an asset type that can be used as a template to spawn new object instances. In addition to prefabs, \textit{MonoBehaviours} are scripts added as a prefab component that are then attached to an object instance of the prefab.\\

For our simulation it is immediately clear that we need to create Drone and Controller prefabs, as they are two entity types that will be interacting with each other. Even if they do not possess any physical or graphical components, they require representation in the simulated world.\\

A Drone prefab would require MonoBehaviours to handle movement and collision avoidance, whereas a Controller prefab will contain scripts to deal with waypoint management, pathfinding and world representation.

\subsubsection{SpatialOS Components}
SpatialOS, like many game engines, works as an Entity-Component-System (ECS). The way to define these the components that entities in the world can have is through the \textit{Schema}. The schema is used to generate code that can then be used by prefab scripts in order to update the SpatialOS state of objects.\\

Each component requires a unique, explicitly defined component ID to distinguish it from other components. Then optionally, we can define properties, events and commands. Properties hold the state of a component, events are used to trigger intra-entity actions, and commands are request-response operations that indicate how different entities communicate with each other.

\begin{sexylisting}[colback=white]{Example Schema - Health}
package improbable.example;

type DamageResponse {}
type DamageRequest {
  uint32 amount = 1;
}

component Health {
  id = 1234;
  uint32 health = 1;
  command DamageResponse damage(DamageRequest);
}
\end{sexylisting}

When spawning an entity, we create a \textit{template} of a defined prefab and add the components that we wish to be on that entity, setting the initial state of component properties in the process. In the case of a Health component we may wish to set \textit{health} of a player to 100, but 20 for little monsters and perhaps 200 for boss fights.\\

\begin{sexylisting}[colback=white]{Creating a Cube Template}
public static Entity CreateCubeTemplate()
{
  return EntityBuilder.Begin()
    .AddPositionComponent(...)
    .AddMetadataComponent(entityType: SimulationSettings.CubePrefabName)
    .SetPersistence(true)
    .SetReadAcl(...)
    .AddComponent(new Rotation.Data(...))
    .Build();
}
\end{sexylisting}

SpatialOS is then tasked with spawning an entity using the template, with the ability to set callbacks for both successful and failed creation attempts.\\

\begin{sexylisting}[colback=white]{Spawning a Cube Object}
var cubeTemplate = EntityTemplateFactory.CreateCubeTemplate();
SpatialOS.Commands.CreateEntity(PositionWriter, cubeTemplate)
  .OnSuccess((obj) =>
    Debug.LogFormat("Created Entity {0}", obj.createdEntityId))
  .OnFailure(() => Debug.LogError("Unable to spawn Cube."));
\end{sexylisting}

\newpage
\section{REVISIT: Implementation of Core Entities}
\subsection{Controller}
A Controller has three distinct components to it which all operate together, two of which are the Global Layer and corresponding Bitmap. A \textit{GridGlobalLayer} script would be used to generate a simple path from start to destination when requested. On initialisation, it would need to populate and update the Bitmap component with details of all the No Fly Zones in the that controller's region of the world.\\

\textit{GridGlobalLayer} itself will be called by \textit{ControllerBehaviour}. This MonoBehaviour is responsible for overseeing the overall operation of a Controller. Spawning drones, calling the pathfinding component, and updating the drone-waypoint mappings are all examples of tasks that this "master" behaviour would have to handle.

\begin{figure}[!hbpt]
 \centering
 \begin{tikzpicture}[node distance = 2cm, auto]
     % Place nodes
     \node [block] (controller) {Controller Behaviour};
     \node [cloud, right of=controller, node distance=5cm] (genpath) {Generate Path};
     \node [block, below of=genpath, node distance=3cm] (globallayer) {Global Layer};
     \node [cloud, left of=globallayer, node distance=5cm] (checknfz) {Check No Fly Zones};
     \node [block, below of=checknfz, node distance=3cm] (bitmap) {Bitmap};
     % \node [cloud, left of=init] (expert) {expert};
     % \node [cloud, right of=init] (system) {system};
     % \node [block, below of=init] (identify) {identify candidate models};
     % \node [block, below of=identify] (evaluate) {evaluate candidate models};
     % \node [block, left of=evaluate, node distance=3cm] (update) {update model};
     % \node [decision, below of=evaluate] (decide) {is best candidate better?};
     % \node [block, below of=decide, node distance=3cm] (stop) {stop};
     % Draw edges
     \path [line] (controller) -- (genpath);
     \path [line] (genpath) -- (globallayer);
     \path [line] (globallayer) -- (checknfz);
     \path [line] (checknfz) -- (bitmap);
     % \path [line] (identify) -- (evaluate);
     % \path [line] (evaluate) -- (decide);
     % \path [line] (decide) -| node [near start] {yes} (update);
     % \path [line] (update) |- (identify);
     % \path [line] (decide) -- node {no}(stop);
     % \path [line,dashed] (expert) -- (init);
     % \path [line,dashed] (system) -- (init);
     % \path [line,dashed] (system) |- (evaluate);
 \end{tikzpicture}
 \caption{Example Component Flow.}
 \label{fig:controller_global_bitmap_flow}
 \end{figure}

 Figure \ref{fig:controller_global_bitmap_flow} describes how the three components slot next to each other, each performing its own set of tasks but maintaining separation of logic and state.

\subsubsection{No Fly Zones}
No Fly Zones are represented in a similar way to AATC, as a list of float vectors with two vectors to track opposing corners of the zone's bounding box. An initial list of No Fly Zones is passed into the Global Layer upon creation of the component, with scope to add and remove from the list if one so wishes.\\
\begin{sexylisting}[colback=white]{NoFlyZone Schema Type}
type NoFlyZone {
  improbable.Vector3f bounding_box_bottom_left = 1;
  improbable.Vector3f bounding_box_top_right = 2;
  list<improbable.Vector3f> vertices = 3;
}
\end{sexylisting}

To complement the C\# class that SpatialOS generates from this, we have implemented a static class (Appendix A) containing methods that are necessary for integrating No Fly Zones with the Global Layer and Bitmap components. These are checks to see if:
\begin{itemize}
  \item a given point is within a zone's bounding box
  \item a given point is within a zone's polygonal area
  \item a given point is within any zone from a given list
\end{itemize}

\subsubsection{Pathfinding and Waypoints}
Once the No Fly Zone data is initialised, it is the controller's job to generate a list of NFZ-avoiding waypoints that a drone will follow to get from start to destination. The controller needs to maintain a list of waypoints for each drone, as well as the next waypoint that each drone will need to go to.\\

This is achieved by keeping a mapping of a drone's ID to it's DroneInfo type, which contains a list of waypoints and the index of the last sent waypoint.

\begin{sexylisting}[colback=white]{Basic Controller Schema}
enum TargetResponseCode {
  SUCCESS = 0;
  WRONG_CONTROLLER = 1;
  JOURNEY_COMPLETE = 2;
}

type TargetRequest {
  EntityId drone_id = 1;
}

type TargetResponse {
  improbable.Vector3f new_target = 1;
  TargetResponseCode success = 2;
}

type DroneInfo {
  int32 next_waypoint = 1;
  list<improbable.Vector3f> waypoints = 2;
}

component Controller {
  id = 1200;
  uint32 max_drone_count = 2;
  map<EntityId, DroneInfo> deliveries_map = 3;
  bool initialised = 4;
  Vector3f top_left = 5;
  Vector3f bottom_right = 6;
  command TargetResponse request_new_target(TargetRequest);
}
\end{sexylisting}

\newpage
When a drone reaches a waypoint it calls the \textit{request\_new\_target} command on its assigned controller, which then locates the DroneInfo of said drone. The \textit{next\_waypoint} field is incremented and the next waypoint is subsequently found and returned to the drone. If the field value is greater than or equal to the length of the list, the drone has clearly reached the final waypoint, its ultimate destination.

\subsubsection{World Bitmap}
In the original AATC implementation, the bitmap was represented as a byte array with each cell representing a 25m x 25m area of the world. Since the only information stored in the bitmap is the presence and proximity to NFZs, the array is sparse and therefore better represented as a map from cell number to GridType.\\

\begin{sexylisting}[colback=white]{Bitmap Component and GridType Enum}
enum GridType {
  OUT = 0;
  IN = 1;
  NEAR = 2;
}

component BitmapComponent {
  id = 1204;
  improbable.Vector3f top_left = 1;
  improbable.Vector3f bottom_right = 2;
  int32 width = 3;
  int32 height = 4;
  int32 grid_width = 7;
  int32 grid_height = 8;
  map<int32, GridType> grid = 5;
  bool initialised = 6;
}
\end{sexylisting}

The benefit of this implementation is that only relevant data is stored in the map, with failure to find a cell in the map implying that the corresponding area of the world is not in or near a No Fly Zone. Since every controller will have a Bitmap component, this design improves memory efficiency and provides more options to the controller. It could simply reduce its memory footprint, deal with larger areas, or reduce the cell size to represent the world with a higher precision.

\subsubsection{Assisting the Reactive Layer}
Although collision avoidance runs on the drone, it can not do the job properly without knowing the closest point to it on a NFZ. This is where the controller comes in and asks the bitmap to return the closest NFZ point. Since NFZs extend across all altitudes, the drone's altitude is used as the y-coordinate of the closest NFZ point.\\

This means that there is now a fourth component to the Controller entity, existing just to deal with requests from drones for the nearest static obstacle. This is shown as the \textit{get\_nearest\_obstacle} command, which returns the \textit{APFObstacle} used by drone-side collision avoidance code.\\

\begin{sexylisting}[colback=white]{Reactive Layer Request Handler}
void GetNearestObstacle(RequestHandle handle)
{
  Vector3f nfz = bitmap.nearestNFZPoint(handle.Request.location);
  APFObstacleType type = APFObstacleType.NO_FLY_ZONE;
  if (nearestNoFlyZone.y < 0)
  {
    type = APFObstacleType.NONE;
  }
  handle.Respond(new ObstacleResponse(new APFObstacle(type, nfz)));
}
\end{sexylisting}

\begin{sexylisting}[colback=white]{Reactive Layer Schema}
package improbable.drone;

import "improbable/vector3.schema";

enum APFObstacleType {
  NONE = 0;
  MANNED_AVIATION = 1;
  DRONE = 2;
  NO_FLY_ZONE = 3;
  HIDDEN_OBSTACLE = 4;
}

type APFObstacle{
  APFObstacleType type = 1;
  improbable.Vector3f position = 2;
}

type ObstacleRequest {
  improbable.Vector3f location = 1;
}

type ObstacleResponse {
  APFObstacle obstacle = 1;
}

component ReactiveLayer {
  id = 1205;
  command ObstacleResponse get_nearest_obstacle(ObstacleRequest);
}
\end{sexylisting}

\subsubsection{Drone Spawning}
For simplicity at this early stage, controllers just generate a path between two random non-NFZ points in the world. They spawn a drone at the first random point and de-spawn each drone once it reaches its final waypoint. For testing purposes collisions were initially disabled as the drones just blindly moved from waypoint to waypoint without any collision avoidance.

\begin{figure}[!hbpt]
  \center
  \includegraphics[width=\linewidth]{img/gdoc1.png}
  \caption{Testing basic controller functionality.}
  \label{fig:basic_controller_test}
\end{figure}

In Figure \ref{fig:basic_controller_test}, we check if the controller returns correct waypoints in order by spawning drones either side of the NFZ and observing if the flight path avoids the obstacle.

\subsection{Drone}
Compared to a controller, a drone is a far less complex entity. It has one main component to maintain state of the drone's next waypoint, current direction, target request status, and maximum speed. The core loop of the \textit{DroneBehaviour} script executes every second, updating SpatialOS with the drone position and then either recalculating direction or requesting a new target, depending on the drone's status.

\begin{sexylisting}[colback=white]{Main Drone Loop (once a second)}
void DroneTick()
{
  if (simulate)
  {
    SendPositionUpdate();
    if (DroneDataWriter.Data.droneStatus == DroneStatus.MOVE)
    {
      apf.Recalculate();
    }
    if (DroneDataWriter.Data.targetPending == TargetPending.WAITING)
    {
      requestNewTarget();
    }
    float distanceToTarget = Vector3.Distance(target, transform.position);
    if (DroneDataWriter.Data.targetPending == TargetPending.REQUEST
      || distanceToTarget < radius)
    {
      requestNewTarget();
    }
  }
}
\end{sexylisting}

Actual movement of the drone occurs in a separate method (Listing 10) \textit{four} times a second. This is done so that the drones visually appear to move smoother and the separation exists to reduce how often we perform the costly tasks of detecting and avoiding nearby drones.\\

Adding this separation will also make it easier down the road when we introduce energy consumption to the drones, because we can then simply switch on the drone status to adjust how much energy has been consumed in the previous time frame. For example, a moving drone will consume more energy than a hovering drone.

\begin{sexylisting}[colback=white]{Drone Movement (four times a second)}
void MoveDrone()
{
  if (simulate)
  {
    if (DroneDataWriter.Data.droneStatus == DroneStatus.MOVE)
    {
      transform.position += direction * DroneDataWriter.Data.speed
        * SimulationSettings.DroneMoveInterval;
    }
  }
}
\end{sexylisting}

\subsubsection{Drone Detection}
In AATC the server knew the location of every drone in the world, which meant that searching for nearby drones would just be a case of iterating through the list of drones and finding the closest one. However, with SpatialOS we delegate this responsiblity to the drones.\\

The naive method would be for each drone to request all SpatialOS entities within a certain radius, and then filtering to find the closest one. The problem with this is that Spatial find queries are extremely expensive as they have to propagate up to the server instance, be executed, and then returned down to individual entities. In a world with 30 drones where each drone is recalculating its direction every second, that's 30 queries that have to be dealt with in tandem every second.\\

Due to the fact that each distributed ``worker'' in SpatialOS is an instance of the Unity game engine, a more practical approach is instead to utilise the engine and perform a simple physics collision check.

Although this is still resource-expensive, it is still quicker than a Spatial search because using native game engine functionality is more performant than manually searching for entities in a radius. By applying a Sphere Collider to each drone, checking for nearby colliders will only ever return instances of drones (Appendix B).

\subsubsection{Collision Detection}
Now that drones are detected easily, a collision is simply the case where a nearby collider is at a distance of less than 1m (2 drone radiuses). In the real world, a controller would detect that two drones have stopped responding and assume a crash occured. For our simulation, we instead add a command for drones to call so the controller knows a collision occurred. \\

\begin{sexylisting}[colback=white]{Reporting a Collision}
type CollisionResponse {}
type CollisionRequest {
  EntityId drone_id = 1;
  EntityId collider_id = 2;
}

command CollisionResponse collision(CollisionRequest);
\end{sexylisting}

Upon successful receipt of a \textit{CollisionRequest}, a controller may then destroy the drone instances to signal that they are not part of the system anymore. Looking ahead, the controller may decide to use this as a trigger to dispatch drone recovery services or apply penalties for crashing drones.

\subsection{Summary}
Now that the Drone and Controller entities have been created and relevant components added, we are able to create drones, send them between random locations, and destroy them upon journey completion.


%**********************************************%
\newpage
\section{REVISIT: Delivery Network Architecture}
As SpatialOS abstracts away all server-to-server connections and raw networking, we can consider any size world as one singular simulation. As explored thus far, our work only needs to go as far as the definition entities and how their components interact with each other. Since pathfinding and drone-controller communication channels have been set up and tested, our efforts now turn to upgrading this system into a scalable, distributed, delivery network.

\subsection{Delivery Destinations}
As per the current design, controllers spawn drones, generate a path between two points and then sequentially provide waypoints the drone should follow to get to its destination. To modify this for a delivery network case, the points should correspond to the locations of the controller and the delivery end point. As controllers are both stationary and responsible for pathfinding, making this change is trivial. The question now: how do controllers find out where the delivery end points are?\\

To resolve this we introduce an Order Generation entity to the simulation, whose sole functionality is to simulate a stream of delivery requests and route these requests to the appropriate controller. The Order Generator may eventually have a load balancing component to it in the future, but for now we simply route each delivery request to the controller closest to the package destination.\\

Figure \ref{fig:delivery_network_overview} shows how the Order Generator fits into the existing model. While controllers and drones represent physical things that will exist in the real world, order generation is a virtual entity that only exists for the purposes of the simulation. If such a system were to go live, the order generation entity would be replaced by the actual stream of orders that real customers would be placing.

\begin{figure}[!hbpt]
 \centering
 \begin{tikzpicture}[node distance = 2cm, auto]
     % Place nodes
     \node [block] (ordergeneration) {Order Generator};
     \node [block, below of=ordergeneration, node distance=3cm] (controller1) {Controller};
     \node [block, left of=controller1, node distance=3cm] (controller2) {Controller};
     \node [block, right of=controller1, node distance=3cm] (controller3) {Controller};
     \node [cloud, below of=controller1, node distance=2cm] (drone1) {Drones};
     \node [cloud, below of=controller2, node distance=2cm] (drone2) {Drones};
     \node [cloud, below of=controller3, node distance=2cm] (drone3) {Drones};
     % \node [cloud, left of=globallayer, node distance=5cm] (checknfz) {Check No Fly Zones};
     % \node [block, below of=checknfz, node distance=3cm] (bitmap) {Bitmap};
     % \node [cloud, left of=init] (expert) {expert};
     % \node [cloud, right of=init] (system) {system};
     % \node [block, below of=init] (identify) {identify candidate models};
     % \node [block, below of=identify] (evaluate) {evaluate candidate models};
     % \node [block, left of=evaluate, node distance=3cm] (update) {update model};
     % \node [decision, below of=evaluate] (decide) {is best candidate better?};
     % \node [block, below of=decide, node distance=3cm] (stop) {stop};
     % Draw edges
     \path [line] (ordergeneration) -- (controller1);
     \path [line] (ordergeneration) -- (controller2);
     \path [line] (ordergeneration) -- (controller3);
     \path [line] (controller1) -- (drone1);
     \path [line] (drone1) -- (controller1);
     \path [line] (controller2) -- (drone2);
     \path [line] (drone2) -- (controller2);
     \path [line] (controller3) -- (drone3);
     \path [line] (drone3) -- (controller3);
     % \path [line] (genpath) -- (globallayer);
     % \path [line] (globallayer) -- (checknfz);
     % \path [line] (checknfz) -- (bitmap);
     % \path [line] (identify) -- (evaluate);
     % \path [line] (evaluate) -- (decide);
     % \path [line] (decide) -| node [near start] {yes} (update);
     % \path [line] (update) |- (identify);
     % \path [line] (decide) -- node {no}(stop);
     % \path [line,dashed] (expert) -- (init);
     % \path [line,dashed] (system) -- (init);
     % \path [line,dashed] (system) |- (evaluate);
 \end{tikzpicture}
 \caption{Delivery Network Flow.}
 \label{fig:delivery_network_overview}
 \end{figure}

\subsection{Basic Order Generation}
Before generating any orders, we must first define the components on the Order Generator and Controller that deal with the sending and receiving of delivery requests.

\subsubsection{Delivery Handler / Scheduler}
Controllers will need a \textit{DeliveryHandler} component to enqueue incoming delivery requests. It follows that there are a finite amount of drones available to deliver goods at a particular point in time, therefore the Delivery Handler's function is to maintain a queue of requests and only provide the next delivery to serve when the \textit{ControllerBehaviour} asks for it.\\

The delivery handler effectively acts as a scheduler for the controller, because it decides the order to return requests in a manner completely transparent to core controller operation. The main loop (Appendix C) now checks if the controller can deploy a drone, asks for the next request, handles the delivery request if the scheduler returns one back, and sends the updated queue state up to SpatialOS.

\newpage
To match up with this functionality the schema defines four basic elements:
\begin{enumerate}
  \item \textit{DeliveryRequest} type
  \item \textit{QueueEntry} type
  \item list of QueueEntry items
  \item \textit{request\_delivery} command
\end{enumerate}

\begin{sexylisting}[colback=white]{Delivery Handler Schema}
type DeliveryResponse {
  bool success = 1;
}

type DeliveryRequest {
  improbable.Vector3f destination = 1;
}

type QueueEntry {
  float timestamp = 1;
  DeliveryRequest request = 2;
}

component DeliveryHandler {
  id = 1201;
  list<QueueEntry> request_queue = 1;
  command DeliveryResponse request_delivery(DeliveryRequest);
}
\end{sexylisting}

We designed it this way so that \textit{DeliveryRequest} data could be extended later if we so wish. Important information about a delivery that will extend to support in the future include package weights, package types and even the priority of said delivery. Being able to extend the \textit{QueueEntry} also may open the door for us to play with more complex scheduling algorithms down the line.

\subsubsection{Order Generator}
Now that we have defined a basic \textit{DeliveryRequest} type, it is the order generator's job to regularly create and send these requests to the appropriate controller. At the moment the request only requires a location vector, so our plan to do this is to generate a random point in the world not in a No Fly Zone, and then find the closest controller to said point.\\

We would like the orders to be generated at regular time intervals such that each Controller receives an order every 30 seconds on average. By distributing controllers evenly such that each one is responsible for similar sized regions of the world, we can infer that the Order Generator works generates requests every $\frac{30}{\#controllers}$ seconds.\\

\begin{sexylisting}[colback=white]{Order Generator Schema}
type ControllerInfo {
  EntityId controller_id = 1;
  improbable.Vector3f location = 2;
}

component OrderGeneratorComponent {
  id = 1300;
  list<improbable.controller.NoFlyZone> zones = 1;
  list<ControllerInfo> controllers = 2;
}
\end{sexylisting}

In order to generate valid requests, we need to know about all the No Fly Zones in the world as well as the location and EntityId of each Controller. Since each controller will be placed at pre-determined locations from a starting snapshot, we know both of these details at snapshot-creation and can subsequently use this to set the starting state of the Order Generator.

\section{TODO: London-Scale Simulation}
\subsection{London Snapshot}
\subsubsection{No Fly Zone Representation}
\subsubsection{Snapshot Generator}
\subsubsection{Voronoi Split}

\subsection{Entity Placement}
\subsubsection{No Fly Zones}
\subsubsection{Controllers}
\subsubsection{Order Generator}



%**********************************************%
\newpage
\section{TODO: Integrating Delivery Economics}
\subsection{Improved Order Generation}
\subsubsection{Package Type \& Weight}
\subsubsection{Delivery Priority}

\subsection{Revenue Model}
% talk about the basic revenue model
% show the table of package type/weight ==> delivery cost
% < 30 mins ==> full price
% 30-60 mins ==> 1/2 price
% > 60 mins ==> free delivery

\subsection{Operating Costs}
\subsubsection{Energy Consumption}
\subsubsection{Penalties}

%**********************************************%
\newpage
\section{TODO: Time-Value of Delivery}
Before considering how to maximise profitability, we introduce the concept of the \textit{Time-Value of Deliveries} (TVD). This is based on the \textit{Time-Value of Data} idea described in Section 2.8. Rather than deal with the fixed pricing model of delivery fees as we see in the current day, we propose a new mechanism where the cost of the delivery is a function of the time taken to do said delivery. This is known as a Time-Value Function (TVF). For example, a user may pay 5 for a delivery that takes less than 30 minutes but be charged nothing if it takes longer than one hour.\\

\subsection{Function Representation}

\subsection{Utilised Functions}
\subsubsection{The Halvening}
\subsubsection{Monotonic Decrease}
\subsubsection{Wildcard}

%**********************************************%
\newpage
\section{TODO: Optimised Scheduling}
% mention pre-emption
\subsection{Shortest Job First}
\subsection{Least Lost Value}

%**********************************************%
\newpage
\section{TODO: Evaluation}
\subsection{Metrics}
\subsubsection{Logging}
\subsubsection{Data Extraction}

\subsection{Comparing Schedulers}
% heavy load vs light load (queue size)
% average profit per Delivery
% total profit
% average rejected potential

\subsection{Large Scale London Simulation}
% screenshot of Snapshot
% large scale voronoi
% lots of graphs/comparisons
% describe how this system works at scale
% architecture works at scale, still with room for improvement

%**********************************************%
\newpage
\section{TODO: Future Work}
% more scheduling types
% switching scheduler on-the-fly
% introducing more TVFs
% comparing more schedulers
% split-scheduling: half on FCFS, half on priority scheduling?
% order generation to be more representative of real-world package distribution
% drone refuelling
% drones not needing to come back to hub after every delivery
% drones making delivering more than 1 package in a round trip
% drones picking up non-hub packages and delivering to non-hub destinations
% inter-controller communication
% pass request from one hub to another hub if there's spare capacity
% more drone types
% more package types
% UnityClient for interactive simulation
% improved data extraction tool

%**********************************************%
\newpage
\section{TODO: Conclusion}



%**********************************************%
\newpage
\addcontentsline{toc}{section}{References}

\printbibliography

% \newpage
\begin{appendices}

\section{Static No Fly Zone Class}
\begin{sexylisting}[colback=white]{Static No Fly Zone Class}
public static class NoFlyZone
{
    private static bool isInPolygon(
      Improbable.Controller.NoFlyZone nfz,
      Improbable.Vector3f point);

    public static bool hasCollidedWith(
      Improbable.Controller.NoFlyZone nfz,
      Improbable.Vector3f point);

    public static bool hasCollidedWithAny(
      List<Improbable.Controller.NoFlyZone> zones,
      Vector3f point);

    public static void setBoundingBoxCoordinates(
      ref Improbable.Controller.NoFlyZone nfz);

    public static bool isPointInTheBoundingBox(
      Improbable.Controller.NoFlyZone nfz,
      Improbable.Vector3f point);
}
\end{sexylisting}

\section{Nearby Drone and Collision Detection}
\begin{sexylisting}[colback=white]{Nearby Drone and Collision Detection}
private void CheckForNearbyDrones(Vector3 dronePos, bool collisionsOn)
{
  nearestDrone.type = APFObstacleType.NONE;
  nearestDrone.position = new Vector3f(0, -1, 0);
  nearestDroneDistance = radiusOfInfluence;

  Collider[] hitColliders = Physics.OverlapSphere(dronePos, apfRadius);
  foreach (Collider hitCollider in hitColliders)
  {
    if (hitCollider.gameObject.EntityId() != gameObject.EntityId())
    {
      float currentDroneDistance
        = Vector3.Distance(hitCollider.transform.position, dronePos);

      if (collisionsOn && currentDroneDistance < 1)
      {
        /* collision reported to controller */
      }

      if (currentDroneDistance < nearestDroneDistance)
      {
        nearestDrone.position
          = hitCollider.transform.position.ToSpatialVector3f();
        nearestDrone.type = APFObstacleType.DRONE;
        nearestDroneDistance = currentDroneDistance;
      }
    }
  }
}
\end{sexylisting}

\section{Main Controller Loop}
\begin{sexylisting}[colback=white]{Main Controller Loop}
void ControllerTick()
{
  if (!ControllerWriter.Data.initialised)
  {
    /* initialise global layer */
  }

  QueueEntry nextRequest;
  if (ReadyForDeployment())
  {
    if (scheduler.GetNextRequest(out nextRequest))
    {
      HandleDeliveryRequest(nextRequest);
    }
  }

  scheduler.UpdateDeliveryRequestQueue();
}
\end{sexylisting}

\end{appendices}
\end{document}
